{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef6e6857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5715279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenterRegressionTNet(nn.Module):\n",
    "    ''' Regression network for center delta. a.k.a. T-Net.\n",
    "    Input:\n",
    "        object_point_cloud:  tensor in shape (B,M,C)\n",
    "            point clouds in 3D mask coordinate\n",
    "        one_hot_vec: tensor in shape (B,3)\n",
    "            length-3 vectors indicating predicted object type\n",
    "    Output:\n",
    "        predicted_center:  tensor in shape (B,3)\n",
    "    '''\n",
    "    def __init__(self, in_dim = 4):\n",
    "        super(CenterRegressionTNet, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(in_dim, 128, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(128, 256, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(256, 512, 1)\n",
    "        self.fc1 = nn.Linear(512 + 3, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 3)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.bn3 = nn.BatchNorm1d(512)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.bn5 = nn.BatchNorm1d(128)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, point_cloud, one_hot_vec):\n",
    "        batchsize = point_cloud.size()[0]\n",
    "        x = point_cloud.transpose(2, 1)\n",
    "        \n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        \n",
    "        x = x.view(-1, 512)\n",
    "        \n",
    "        x = torch.cat([one_hot_vec, x], 1)\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        predicted_center = self.fc3(x)\n",
    "        \n",
    "        return predicted_center\n",
    "\n",
    "\n",
    "\n",
    "class PointNetfeat(nn.Module):\n",
    "    def __init__(self, in_dim = 4, global_feat = True):\n",
    "        super(PointNetfeat, self).__init__()\n",
    "        self.in_dim  = in_dim\n",
    "        self.conv1 = torch.nn.Conv1d(self.in_dim, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 64, 1)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(64, 64, 1)\n",
    "        self.conv4 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv5 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)        \n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.bn5 = nn.BatchNorm1d(1024)\n",
    "        self.global_feat = global_feat\n",
    "\n",
    "\n",
    "    def forward(self, point_cloud, one_hot_vec):\n",
    "        x = point_cloud.transpose(2, 1)\n",
    "        n_pts = x.size()[2]\n",
    "\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        pointfeat = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = pointfeat\n",
    "        \n",
    "        \n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.bn5(self.conv5(x))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "        x = torch.cat([one_hot_vec, x], 1)\n",
    "        if self.global_feat:\n",
    "            return x\n",
    "        else:\n",
    "            x = x.view(-1, 1024 + 3, 1).repeat(1, 1, n_pts)\n",
    "            x = torch.cat([x, pointfeat], 1)\n",
    "            x = x.transpose(2, 1)\n",
    "            return x\n",
    "\n",
    "\n",
    "class PointNetSegmentation(nn.Module):\n",
    "    ''' 3D instance segmentation PointNet v1 network.\n",
    "    Input:\n",
    "        point_cloud:  tensor in shape (B,N,4)\n",
    "            frustum point clouds with XYZ and intensity in point channels\n",
    "            XYZs are in frustum coordinate\n",
    "        one_hot_vec:  tensor in shape (B,3)\n",
    "            length-3 vectors indicating predicted object type\n",
    "        bn_decay:  float scalar\n",
    "        end_points: dict\n",
    "    Output:\n",
    "        logits:  tensor in shape (B,N,2), scores for bkg/clutter and object'''        \n",
    "    def __init__(self, k = 2):\n",
    "        super(PointNetSegmentation, self).__init__()\n",
    "        self.k = k\n",
    "        self.feat = PointNetfeat(global_feat=False)\n",
    "        self.conv1 = torch.nn.Conv1d(1088 + 3, 512, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(512, 256, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(256, 128, 1)\n",
    "        self.conv4 = torch.nn.Conv1d(128, 128, 1)\n",
    "        self.conv5 = torch.nn.Conv1d(128, self.k, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(p = 0.5)\n",
    "\n",
    "    def forward(self, point_cloud, one_hot_vec):\n",
    "        batchsize = point_cloud.size()[0]\n",
    "        n_pts = point_cloud.size()[1]\n",
    "        x = self.feat(point_cloud, one_hot_vec)\n",
    "        x = x.transpose(2, 1)\n",
    "        # size here is Bx1091xN\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.dropout1(x) \n",
    "        x = self.conv5(x)\n",
    "        x = x.transpose(2,1).contiguous()\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b5cc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HEADING_BIN = 12\n",
    "NUM_SIZE_CLUSTER = 8 # one cluster for each type\n",
    "NUM_OBJECT_POINT = 512\n",
    "\n",
    "def point_cloud_masking(point_cloud, segmentation_results, end_points):\n",
    "    ''' Select point cloud with predicted 3D mask,\n",
    "    translate coordinates to the masked points centroid.\n",
    "\n",
    "    Input:\n",
    "        point_cloud:  tensor in shape (B,N,C)\n",
    "        segmentation_results:  tensor in shape (B,N,2)\n",
    "        xyz_only: boolean, if True only return XYZ channels\n",
    "    Output:\n",
    "        object_point_cloud:  tensor in shape (B,M,3)\n",
    "            for simplicity we only keep XYZ here\n",
    "            M = NUM_OBJECT_POINT as a hyper-parameter\n",
    "        mask_xyz_mean:  tensor in shape (B,3)\n",
    "    '''\n",
    "\n",
    "    batch_size = point_cloud.size()[0]\n",
    "    num_point = point_cloud.size()[1]\n",
    "    mask_bool =(segmentation_results[:,:,0] < segmentation_results[:,:,1])\n",
    "    mask = mask_bool.float()\n",
    "#     print(mask.size())\n",
    "    mask_count = torch.sum(mask, 1, keepdim=True).repeat(1, 1, 3)\n",
    "    mask_count = mask_count.view(batch_size, 1, 3)\n",
    "    point_cloud_xyz = point_cloud[:,:,:3] # only xyz\n",
    "    end_points['mask'] = mask\n",
    "#     print(point_cloud_xyz.size(), mask_bool.size())\n",
    "    mask_xyz_mean = (mask.unsqueeze(2).repeat(1,1,3)*point_cloud_xyz).sum(dim = 1, keepdim= True)\n",
    "    mask_xyz_mean = mask_xyz_mean/torch.clamp(mask_count, min = 1)\n",
    "#     print(mask_xyz_mean.size())\n",
    "    point_cloud_xyz_stage1 = point_cloud_xyz - mask_xyz_mean.repeat(1, num_point, 1)\n",
    "#     print(point_cloud_xyz_stage1.size())\n",
    "\n",
    "    point_cloud_stage1 = point_cloud_xyz_stage1\n",
    "    npoints=NUM_OBJECT_POINT\n",
    "    non_zero_indices = torch.cat([torch.where((mask>0.5))[0].unsqueeze(1), torch.where((mask>0.5))[1].unsqueeze(1)],1)\n",
    "    object_pc = torch.zeros(batch_size, npoints, 3)\n",
    "    for i in range(batch_size):\n",
    "\n",
    "        all_points_indices = non_zero_indices[non_zero_indices[:,0] == i][:,1]\n",
    "        if all_points_indices.size()[0] > 0:\n",
    "\n",
    "            segmented_points = point_cloud_stage1[i][all_points_indices]\n",
    "            if segmented_points.size()[0] >= npoints:\n",
    "                perm = torch.randperm(segmented_points.size(0))\n",
    "                object_pc[i] = segmented_points[perm[:npoints]]\n",
    "            else:\n",
    "                choice = np.random.choice(segmented_points.size(0),\n",
    "                    npoints - segmented_points.size(0), replace=True)\n",
    "                choice = np.concatenate((np.arange(segmented_points.size(0)), choice))\n",
    "                np.random.shuffle(choice)\n",
    "                object_pc[i] = segmented_points[choice]\n",
    "    return object_pc, mask_xyz_mean.squeeze(1), end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c68ffd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet3DboxEstimationNet(nn.Module):\n",
    "    ''' 3D Box Estimation PointNet v1 network.\n",
    "    Input:\n",
    "        object_point_cloud: TF tensor in shape (B,M,C)\n",
    "            point clouds in object coordinate\n",
    "        one_hot_vec: TF tensor in shape (B,3)\n",
    "            length-3 vectors indicating predicted object type\n",
    "    Output:\n",
    "        output: TF tensor in shape (B,3+NUM_HEADING_BIN*2+NUM_SIZE_CLUSTER*4)\n",
    "            including box centers, heading bin class scores and residuals,\n",
    "            and size cluster scores and residuals\n",
    "    '''\n",
    "    def __init__(self, in_dim = 3):\n",
    "        super(PointNet3DboxEstimationNet, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(in_dim, 128, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(128, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 256, 1)\n",
    "        self.conv4 = torch.nn.Conv1d(256, 512, 1)\n",
    "        self.fc1 = nn.Linear(512 + 3, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 3+NUM_HEADING_BIN*2+NUM_SIZE_CLUSTER*4)\n",
    "        \n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(512)\n",
    "        self.bn6 = nn.BatchNorm1d(256)\n",
    " \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, point_cloud, one_hot_vec):\n",
    "        batchsize = point_cloud.size()[0]\n",
    "        x = point_cloud.transpose(2, 1)\n",
    "        \n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        \n",
    "        x = x.view(-1, 512)\n",
    "        \n",
    "        x = torch.cat([x, one_hot_vec], 1)\n",
    "        x = F.relu(self.bn5(self.fc1(x)))\n",
    "        x = F.relu(self.bn6(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcb92d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # masked_pc.size(), mean_xyz.size(), end_points\n",
    "# print(masked_pc - outstn.unsqueeze(1) )\n",
    "NUM_HEADING_BIN = 12\n",
    "NUM_SIZE_CLUSTER = 8 # one cluster for each type\n",
    "NUM_OBJECT_POINT = 512\n",
    "g_type2class={'Car':0, 'Van':1, 'Truck':2, 'Pedestrian':3,\n",
    "              'Person_sitting':4, 'Cyclist':5, 'Tram':6, 'Misc':7}\n",
    "g_class2type = {g_type2class[t]:t for t in g_type2class}\n",
    "g_type2onehotclass = {'Car': 0, 'Pedestrian': 1, 'Cyclist': 2}\n",
    "g_type_mean_size = {'Car': np.array([3.88311640418,1.62856739989,1.52563191462]),\n",
    "                    'Van': np.array([5.06763659,1.9007158,2.20532825]),\n",
    "                    'Truck': np.array([10.13586957,2.58549199,3.2520595]),\n",
    "                    'Pedestrian': np.array([0.84422524,0.66068622,1.76255119]),\n",
    "                    'Person_sitting': np.array([0.80057803,0.5983815,1.27450867]),\n",
    "                    'Cyclist': np.array([1.76282397,0.59706367,1.73698127]),\n",
    "                    'Tram': np.array([16.17150617,2.53246914,3.53079012]),\n",
    "                    'Misc': np.array([3.64300781,1.54298177,1.92320313])}\n",
    "g_mean_size_arr = np.zeros((NUM_SIZE_CLUSTER, 3)) # size clustrs\n",
    "for i in range(NUM_SIZE_CLUSTER):\n",
    "    g_mean_size_arr[i,:] = g_type_mean_size[g_class2type[i]]\n",
    "\n",
    "def parse_output_to_tensors(output, end_points):\n",
    "    ''' Parse batch output to separate tensors (added to end_points)\n",
    "    Input:\n",
    "        output: TF tensor in shape (B,3+2*NUM_HEADING_BIN+4*NUM_SIZE_CLUSTER)\n",
    "        end_points: dict\n",
    "    Output:\n",
    "        end_points: dict (updated)\n",
    "    '''\n",
    "    batch_size = output.size(0)\n",
    "#     center = tf.slice(output, [0,0], [-1,3])\n",
    "    center = output[:,:3]\n",
    "    \n",
    "    end_points['center_boxnet'] = center\n",
    "\n",
    "    heading_scores = output[:,3:3 + NUM_HEADING_BIN]\n",
    "    heading_residuals_normalized = output[:,3 + NUM_HEADING_BIN: 3 + NUM_HEADING_BIN + NUM_HEADING_BIN]\n",
    "    end_points['heading_scores'] = heading_scores # BxNUM_HEADING_BIN\n",
    "    end_points['heading_residuals_normalized'] = \\\n",
    "        heading_residuals_normalized # BxNUM_HEADING_BIN (-1 to 1)\n",
    "    end_points['heading_residuals'] = \\\n",
    "        heading_residuals_normalized * (np.pi/NUM_HEADING_BIN) # BxNUM_HEADING_BIN\n",
    "    \n",
    "    size_scores = output[:,3+NUM_HEADING_BIN*2:3+NUM_HEADING_BIN*2+NUM_SIZE_CLUSTER] # BxNUM_SIZE_CLUSTER\n",
    "    size_residuals_normalized = output[:,3+NUM_HEADING_BIN*2+NUM_SIZE_CLUSTER:3+NUM_HEADING_BIN*2+4*NUM_SIZE_CLUSTER]\n",
    "    size_residuals_normalized = size_residuals_normalized.view(batch_size,NUM_SIZE_CLUSTER,3)# BxNUM_SIZE_CLUSTERx3\n",
    "    end_points['size_scores'] = size_scores\n",
    "    end_points['size_residuals_normalized'] = size_residuals_normalized\n",
    "    end_points['size_residuals'] = size_residuals_normalized * \\\n",
    "        torch.from_numpy(g_mean_size_arr).unsqueeze(0)\n",
    "\n",
    "    return end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9e1c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrustrumPointNent_v1(nn.Module):\n",
    "    ''' Frustum PointNets model. The model predict 3D object masks and\n",
    "    amodel bounding boxes for objects in frustum point clouds.\n",
    "\n",
    "    Input:\n",
    "        point_cloud: TF tensor in shape (B,N,4)\n",
    "            frustum point clouds with XYZ and intensity in point channels\n",
    "            XYZs are in frustum coordinate\n",
    "        one_hot_vec: TF tensor in shape (B,3)\n",
    "            length-3 vectors indicating predicted object type\n",
    "        is_training: TF boolean scalar\n",
    "        bn_decay: TF float scalar\n",
    "    Output:\n",
    "        end_points: dict (map from name strings to tensors)'''\n",
    "    \n",
    "    def __init__(self, in_dim = 4):\n",
    "        super(FrustrumPointNent_v1, self).__init__()\n",
    "        self.segmentationNet = PointNetSegmentation(k = 2)\n",
    "        self.centerRegressionNet = CenterRegressionTNet(in_dim = 3)\n",
    "        self.amodalBoxEstimationNet = PointNet3DboxEstimationNet(in_dim = 3)\n",
    "        \n",
    "\n",
    "    def forward(self, point_cloud, one_hot_vec):\n",
    "        end_points = dict()\n",
    "        \n",
    "        # Segmentation\n",
    "        logits = self.segmentationNet(point_cloud, one_hot_vec)\n",
    "        end_points['mask_logits'] = logits\n",
    "        \n",
    "        # point cloud masking according to results of object_\n",
    "        object_point_cloud_xyz, mask_xyz_mean, end_points = point_cloud_masking(point_cloud, logits, end_points)\n",
    "        print()\n",
    "        # finding the center of the object\n",
    "        center_delta = self.centerRegressionNet(object_point_cloud_xyz, one_hot_vec)\n",
    "        stage1_center = center_delta + mask_xyz_mean # Bx3\n",
    "        end_points['stage1_center'] = stage1_center\n",
    "        \n",
    "        # Get object point cloud in object coordinate\n",
    "        object_point_cloud_xyz_new = object_point_cloud_xyz - center_delta.unsqueeze(1)\n",
    "        \n",
    "        # Amodel Box Estimation PointNet\n",
    "        output = self.amodalBoxEstimationNet(object_point_cloud_xyz_new, one_hot_vec)\n",
    "        \n",
    "        end_points = parse_output_to_tensors(output, end_points)\n",
    "        end_points['center'] = end_points['center_boxnet'] + stage1_center # Bx3\n",
    "\n",
    "        return end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c3ce548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'headings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-49f23f99a974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mheadings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_box3d_corners_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheadings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# l = sizes[:, 0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(sizes.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# torch.cat([l/2,l/2,-l/2,-l/2,l/2,l/2,-l/2,-l/2]).size()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'headings' is not defined"
     ]
    }
   ],
   "source": [
    "headings.size()\n",
    "get_box3d_corners_helper(centers, headings, sizes).size()\n",
    "# l = sizes[:, 0]\n",
    "# print(sizes.size())\n",
    "# torch.cat([l/2,l/2,-l/2,-l/2,l/2,l/2,-l/2,-l/2]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "145a3ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = torch.randn(250,3)\n",
    "headings = torch.randn(250)\n",
    "sizes = torch.randn(250,3)\n",
    "\n",
    "\n",
    "center = torch.randn(250,3)\n",
    "heading_residuals = torch.randn(250,NUM_HEADING_BIN)\n",
    "size_residuals = torch.randn(250,NUM_SIZE_CLUSTER, 3) \n",
    "def get_box3d_corners_helper(centers, headings, sizes):\n",
    "    \"\"\" TF layer. Input: (N,3), (N,), (N,3), Output: (N,8,3) \"\"\"\n",
    "    #print '-----', centers\n",
    "    N = centers.size(0)\n",
    "#     l = tf.slice(sizes, [0,0], [-1,1]) # (N,1)\n",
    "    l = sizes[:, 0:1]\n",
    "    w = sizes[:, 1:2] # (N,1)\n",
    "    h = sizes[:, 2:3] # (N,1)\n",
    "#     print (l.size())\n",
    "    x_corners = torch.cat([l/2,l/2,-l/2,-l/2,l/2,l/2,-l/2,-l/2], dim=1) # (N,8)\n",
    "    y_corners = torch.cat([h/2,h/2,h/2,h/2,-h/2,-h/2,-h/2,-h/2], dim=1) # (N,8)\n",
    "    z_corners = torch.cat([w/2,-w/2,-w/2,w/2,w/2,-w/2,-w/2,w/2], dim=1) # (N,8)\n",
    "#     print (z_corners.size())\n",
    "    corners = torch.cat([x_corners.unsqueeze(1), y_corners.unsqueeze(1), z_corners.unsqueeze(1)], axis=1) # (N,3,8)\n",
    "#     print (corners.size())\n",
    "    c = torch.cos(headings)\n",
    "    s = torch.sin(headings)\n",
    "#     print (s.size())\n",
    "    ones = torch.ones([N], dtype=torch.float32)\n",
    "    zeros = torch.zeros([N], dtype=torch.float32)\n",
    "    row1 = torch.stack([c,zeros,s], axis=1) # (N,3)\n",
    "#     print(zeros.size(), row1.size())\n",
    "    row2 = torch.stack([zeros,ones,zeros], axis=1)\n",
    "    row3 = torch.stack([-s,zeros,c], axis=1)\n",
    "    R = torch.cat([row1.unsqueeze(1), row2.unsqueeze(1), row3.unsqueeze(1)], axis=1) # (N,3,3)\n",
    "#     print (row1.size(),R.size(), N)\n",
    "    corners_3d = torch.matmul(R, corners) # (N,3,8)\n",
    "    corners_3d += centers.unsqueeze(2).repeat(1,1,8)  # (N,3,8)\n",
    "#     print(corners_3d.size())\n",
    "#     tf.tile(tf.expand_dims(centers,2), [1,1,8])\n",
    "    corners_3d = corners_3d.transpose(2,1) # (N,8,3)\n",
    "#      tf.transpose(corners_3d, perm=[0,2,1])\n",
    "    return corners_3d\n",
    "\n",
    "\n",
    "def get_box3d_corners(center, heading_residuals, size_residuals):\n",
    "    \"\"\" TF layer.\n",
    "    Inputs:\n",
    "        center: (B,3)\n",
    "        heading_residuals: (B,NH)\n",
    "        size_residuals: (B,NS,3)\n",
    "    Outputs:\n",
    "        box3d_corners: (B,NH,NS,8,3) tensor\n",
    "    \"\"\"\n",
    "    batch_size = center.size(0)\n",
    "    heading_bin_centers = torch.from_numpy(np.arange(0,2*np.pi,2*np.pi/NUM_HEADING_BIN)) # (NH,)\n",
    "    headings = heading_residuals + heading_bin_centers.unsqueeze(0) # (B,NH)\n",
    "#     print(headings.size())\n",
    "    mean_sizes = torch.from_numpy(g_mean_size_arr).unsqueeze(0) + size_residuals # (B,NS,1)\n",
    "    sizes = mean_sizes + size_residuals # (B,NS,3)\n",
    "#     sizes = tf.tile(tf.expand_dims(sizes,1), [1,NUM_HEADING_BIN,1,1]) # (B,NH,NS,3)\n",
    "    sizes = sizes.unsqueeze(1).repeat(1,NUM_HEADING_BIN,1,1)\n",
    "#     print(sizes.size())\n",
    "#     headings = tf.tile(tf.expand_dims(headings,-1), [1,1,NUM_SIZE_CLUSTER]) # (B,NH,NS)\n",
    "    headings = headings.unsqueeze(-1).repeat(1,1,NUM_SIZE_CLUSTER)\n",
    "#     print(headings.size())\n",
    "#     centers = tf.tile(tf.expand_dims(tf.expand_dims(center,1),1), [1,NUM_HEADING_BIN, NUM_SIZE_CLUSTER,1]) # (B,NH,NS,3)\n",
    "    centers = center.unsqueeze(1).unsqueeze(1).repeat(1,NUM_HEADING_BIN, NUM_SIZE_CLUSTER,1)\n",
    "#     print(centers.size())\n",
    "    N = batch_size*NUM_HEADING_BIN*NUM_SIZE_CLUSTER\n",
    "    corners_3d = get_box3d_corners_helper(torch.reshape(centers, [N,3]), torch.reshape(headings, [N]), torch.reshape(sizes, [N,3]))\n",
    "#     print(corners_3d.size())\n",
    "    return torch.reshape(corners_3d, [batch_size, NUM_HEADING_BIN, NUM_SIZE_CLUSTER, 8, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "283827d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(error, delta):\n",
    "    abs_error = torch.abs(error)\n",
    "    quadratic = torch.minimum(abs_error,  torch.full_like(abs_error,  delta))\n",
    "#     print(quadratic)\n",
    "    linear = (abs_error - quadratic)\n",
    "#     print(linear)\n",
    "    losses = 0.5 * quadratic**2 + delta * linear\n",
    "#     print(losses)\n",
    "    return torch.mean(losses)\n",
    "\n",
    "\n",
    "\n",
    "def get_loss(mask_label, center_label, \\\n",
    "             heading_class_label, heading_residual_label, \\\n",
    "             size_class_label, size_residual_label, \\\n",
    "             end_points, \\\n",
    "             writer, \\\n",
    "             step, \\\n",
    "             corner_loss_weight=10.0, \\\n",
    "             box_loss_weight=1.0):\n",
    "    ''' Loss functions for 3D object detection.\n",
    "    Input:\n",
    "        mask_label: TF int32 tensor in shape (B,N)\n",
    "        center_label: TF tensor in shape (B,3)\n",
    "        heading_class_label: TF int32 tensor in shape (B,) \n",
    "        heading_residual_label: TF tensor in shape (B,) \n",
    "        size_class_label: TF tensor int32 in shape (B,)\n",
    "        size_residual_label: TF tensor  in shape (B,3)\n",
    "        end_points: dict, outputs from our model\n",
    "        corner_loss_weight: float scalar\n",
    "        box_loss_weight: float scalar\n",
    "    Output:\n",
    "        total_loss: TF scalar tensor\n",
    "            the total_loss is also added to the losses collection\n",
    "    '''\n",
    "    # 3D Segmentation loss\n",
    "#     mask_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\\\n",
    "#         logits=end_points['mask_logits'], labels=mask_label))\n",
    "#     tf.summary.scalar('3d mask loss', mask_loss)\n",
    "    loss3d = nn.CrossEntropyLoss(reduction ='mean')\n",
    "    mask_loss = loss(end_points['mask_logits'].transpose(2,1), (mask_label) )\n",
    "\n",
    "    # Center regression losses\n",
    "#     center_dist = tf.norm(center_label - end_points['center'], axis=-1)\n",
    "    center_dist = torch.norm((center_label) - end_points['center'], dim=-1)\n",
    "    \n",
    "    center_loss = huber_loss(center_dist, delta=2.0)\n",
    "#     tf.summary.scalar('center loss', center_loss)\n",
    "    stage1_center_dist = torch.norm((center_label) - \\\n",
    "        end_points['stage1_center'], dim=-1)\n",
    "    stage1_center_loss = huber_loss((stage1_center_dist), delta=1.0)\n",
    "#     tf.summary.scalar('stage1 center loss', stage1_center_loss)\n",
    "\n",
    "    # Heading loss\n",
    "    loss_heading = nn.CrossEntropyLoss(reduction ='mean')\n",
    "#     heading_class_loss = tf.reduce_mean( \\\n",
    "#         tf.nn.sparse_softmax_cross_entropy_with_logits( \\\n",
    "#         logits=end_points['heading_scores'], labels=heading_class_label))\n",
    "    heading_class_loss = loss(end_points['heading_scores'], (heading_class_label) )\n",
    "#     tf.summary.scalar('heading class loss', heading_class_loss)\n",
    "    hcls_onehot = torch.nn.functional.one_hot(heading_class_label, NUM_HEADING_BIN) # BxNUM_HEADING_BIN\n",
    "#     hcls_onehot = tf.one_hot(heading_class_label,\n",
    "#         depth=NUM_HEADING_BIN,\n",
    "#         on_value=1, off_value=0, axis=-1) \n",
    "    heading_residual_normalized_label = \\\n",
    "        heading_residual_label / (np.pi/NUM_HEADING_BIN)\n",
    "    temp_val = torch.sum(end_points['heading_residuals_normalized']*hcls_onehot.double(), dim=1)\n",
    "    heading_residual_normalized_loss = huber_loss( temp_val - (heading_residual_normalized_label), delta=1.0)\n",
    "#     tf.summary.scalar('heading residual normalized loss',\n",
    "#         heading_residual_normalized_loss)\n",
    "\n",
    "    # Size loss\n",
    "    loss_size = nn.CrossEntropyLoss(reduction ='mean')\n",
    "    size_class_loss = loss(end_points['heading_scores'], (size_class_label) )\n",
    "#     size_class_loss = tf.reduce_mean( \\\n",
    "#         tf.nn.sparse_softmax_cross_entropy_with_logits( \\\n",
    "#         logits=end_points['size_scores'], labels=size_class_label))\n",
    "#     tf.summary.scalar('size class loss', size_class_loss)\n",
    "\n",
    "    scls_onehot = torch.nn.functional.one_hot(size_class_label, NUM_SIZE_CLUSTER) # BxNUM_SIZE_CLUSTER\n",
    "    scls_onehot_tiled = scls_onehot.double().unsqueeze(-1).repeat(1, 1, 3)\n",
    "#     tf.tile(tf.expand_dims( \\\n",
    "#         tf.to_float(scls_onehot), -1), [1,1,3]) # BxNUM_SIZE_CLUSTERx3\n",
    "    predicted_size_residual_normalized = torch.sum( \\\n",
    "        end_points['size_residuals_normalized']*scls_onehot_tiled, dim=[1]) # Bx3\n",
    "\n",
    "    mean_size_arr_expand = torch.from_numpy(g_mean_size_arr).unsqueeze(0) # 1xNUM_SIZE_CLUSTERx3\n",
    "#     mean_size_label = tf.reduce_sum( \\\n",
    "#         scls_onehot_tiled * mean_size_arr_expand, axis=[1]) # Bx3\n",
    "    mean_size_label = torch.sum(scls_onehot_tiled * mean_size_arr_expand, dim=[1]) # Bx3\n",
    "    size_residual_label_normalized = size_residual_label / mean_size_label\n",
    "    size_normalized_dist = torch.norm( \\\n",
    "        size_residual_label_normalized - predicted_size_residual_normalized, dim=-1)\n",
    "    size_residual_normalized_loss = huber_loss(size_normalized_dist, delta=1.0)\n",
    "#     tf.summary.scalar('size residual normalized loss',\n",
    "#         size_residual_normalized_loss)\n",
    "\n",
    "    # Corner loss\n",
    "    # We select the predicted corners corresponding to the \n",
    "    # GT heading bin and size cluster.\n",
    "    corners_3d = get_box3d_corners(end_points['center'],\n",
    "        end_points['heading_residuals'],\n",
    "        end_points['size_residuals']) # (B,NH,NS,8,3)\n",
    "#     gt_mask = tf.tile(tf.expand_dims(hcls_onehot, 2), [1,1,NUM_SIZE_CLUSTER]) * \\\n",
    "#         tf.tile(tf.expand_dims(scls_onehot,1), [1,NUM_HEADING_BIN,1]) # (B,NH,NS)\n",
    "    gt_mask = hcls_onehot.unsqueeze(2).repeat(1,1,NUM_SIZE_CLUSTER)* scls_onehot.unsqueeze(1).repeat(1,NUM_HEADING_BIN,1)\n",
    "    \n",
    "#     corners_3d_pred = tf.reduce_sum( \\\n",
    "#         tf.to_float(tf.expand_dims(tf.expand_dims(gt_mask,-1),-1)) * corners_3d,\n",
    "#         axis=[1,2]) # (B,8,3)\n",
    "    corners_3d_pred = torch.sum(gt_mask.unsqueeze(-1).unsqueeze(-1).double()*corners_3d, axis=[1,2])\n",
    "    \n",
    "    heading_bin_centers = torch.from_numpy(np.arange(0,2*np.pi,2*np.pi/NUM_HEADING_BIN)) # (NH,)\n",
    "#     heading_label = tf.expand_dims(heading_residual_label,1) + \\\n",
    "#         tf.expand_dims(heading_bin_centers, 0) # (B,NH)\n",
    "    heading_label = (heading_residual_label.unsqueeze(1) + heading_bin_centers.unsqueeze(0))\n",
    "    \n",
    "    heading_label = torch.sum((hcls_onehot.double())*heading_label, 1)\n",
    "    mean_sizes = torch.from_numpy(g_mean_size_arr).unsqueeze(0) # (1,NS,3)\n",
    "    \n",
    "    size_label = mean_sizes + size_residual_label.unsqueeze(1)   # (1,NS,3) + (B,1,3) = (B,NS,3)\n",
    "#     print(mean_sizes.size(), size_residual_label.size(), size_label.size())\n",
    "    size_label = torch.sum(scls_onehot.double().unsqueeze(-1)*size_label, axis=[1]) # (B,3)\n",
    "    corners_3d_gt = get_box3d_corners_helper( \\\n",
    "        center_label, heading_label, size_label) # (B,8,3)\n",
    "    corners_3d_gt_flip = get_box3d_corners_helper( \\\n",
    "        center_label, heading_label+np.pi, size_label) # (B,8,3)\n",
    "\n",
    "    corners_dist = torch.minimum(torch.norm(corners_3d_pred - corners_3d_gt, dim=-1),\n",
    "        torch.norm(corners_3d_pred - corners_3d_gt_flip, dim=-1))\n",
    "    corners_loss = huber_loss(corners_dist, delta=1.0) \n",
    "#     tf.summary.scalar('corners loss', corners_loss)\n",
    "\n",
    "    # Weighted sum of all losses\n",
    "    total_loss = mask_loss + box_loss_weight * (center_loss + \\\n",
    "        heading_class_loss + size_class_loss + \\\n",
    "        heading_residual_normalized_loss*20 + \\\n",
    "        size_residual_normalized_loss*20 + \\\n",
    "        stage1_center_loss + \\\n",
    "        corner_loss_weight*corners_loss)\n",
    "#     tf.add_to_collection('losses', total_loss)\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# get_loss(mask_label, center_label, \\\n",
    "#      heading_class_label, heading_residual_label, \\\n",
    "#      size_class_label, size_residual_label, \\\n",
    "#      end_points, \\\n",
    "#      None, \\\n",
    "#      corner_loss_weight=10.0, \\\n",
    "#      box_loss_weight=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91271434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs are point cloud BxNx4 and one-hot vector Bx3\n",
      "Center Regression Net torch.Size([32, 3])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sim_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e029edf2dc14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPointNetSegmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Segmentation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sim_data' is not defined"
     ]
    }
   ],
   "source": [
    "sim_data_point_cloud = Variable(torch.rand(32,2500,4))\n",
    "one_hot = Variable(torch.rand(32,3))\n",
    "print(\"Inputs are point cloud BxNx4 and one-hot vector Bx3\")\n",
    "trans = CenterRegressionTNet()\n",
    "outstn = trans(sim_data_point_cloud, one_hot)\n",
    "print('Center Regression Net', outstn.size())\n",
    "\n",
    "\n",
    "seg = PointNetSegmentation(k = 2)\n",
    "out = seg(sim_data, one_hot)\n",
    "print('Segmentation', out.size())\n",
    "\n",
    "\n",
    "frustrum = FrustrumPointNent_v1()\n",
    "end_points = frustrum(sim_data_point_cloud, one_hot)\n",
    "print(\"All outputs:\")\n",
    "for key in end_points:\n",
    "    print(\"Size of %s :\" %key, end_points[key].size() , type(end_points[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dbb741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8825d542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
